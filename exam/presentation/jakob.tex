

\section{Tiling in step 2}

\begin{frame}[fragile]{Futhark code and semantics}
\begin{minted}{haskell}
  let Xsqr = intrinsics.opaque <|
             map (matmul_filt Xh Xth) Yh
\end{minted}
\texttt{matmul\_filt}: Matrix-matrix multiplication with filtering vector.
Specifically:
\[
  C[i,j]=\sum\limits_{k=0}^{n-1} A[i,k]\cdot B[k,j]\cdot f_k
\]
and
\[
  f_k =
  \begin{cases}
    0 & \mbox{ if } y[k] \mbox{ is \texttt{NaN}} \\
    1 & \mbox{ otherwise }
  \end{cases}
\]
where \(y\) is a row from \texttt{Yh}, \(A\) is \texttt{Xh}, \(B\) is
\texttt{Xth}, and \(C\) is a matrix in \texttt{Xsqr}.
%XXX: Mention dimensions?
\end{frame}

\begin{frame}[fragile]{How can this be implemented?}
  Naively:
\begin{minted}[linenos,fontsize=\scriptsize]{c}
for (int i = 0; i < m; i++) {           // blockIdx.x
  for (int y = 0; y < k2p2; y++) {      // threadIdx.y
    for (int x = 0; x < k2p2; x++) {    // threadIdx.x
      float accum = 0.0;
      for (int l = 0; l < n; l++) {     // sequential
        if (!isnan(Yh[i,l])) {
          accum += Xh[y,l] * Xth[l,x];
        }
      }
      Xsqr[i, y, x] = accum;
    }
  }
}
\end{minted}
Many accesses to same elements of \texttt{Xh} and \texttt{Xth}.
Good opportunity for optimizing temporal locality through tiling.
\end{frame}

\begin{frame}[fragile]{Applying tiling transformations}

\begin{minipage}{.55\textwidth}
\begin{minted}[linenos,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {
  for (int ii = 0; ii < T; ii++) {
    for (int y = 0; y < k2p2; y++) {
      for (int x = 0; x < k2p2; x++) {
        float accum = 0.0;
        for (int l = 0; l < n; l++) {
          if (!isnan(Yh[i+ii,l])) {
            accum += Xh[y,l] * Xth[l,x];
          }
        }
        Xsqr[i+ii, y, x] = accum;
      }
    }
  }
}
\end{minted}
\begin{center}
  Stripmine
\end{center}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \begin{minted}[linenos,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {
  for (int y = 0; y < k2p2; y++) {
    for (int x = 0; x < k2p2; x++) {
      for (int ii = 0; ii < T; ii++) {
        float accum = 0.0;
        for (int l = 0; l < n; l++) {
          if (!isnan(Yh[i+ii,l])) {
            accum += Xh[y,l] * Xth[l,x];
          }
        }
        Xsqr[i+ii, y, x] = accum;
      }
    }
  }
}
\end{minted}
\begin{center}
  Interchange
\end{center}
\end{minipage}
\end{frame}

\begin{frame}[fragile]{Applying tiling transformations}
\begin{minipage}{.55\textwidth}
\begin{minted}[linenos,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {
  for (int y = 0; y < k2p2; y++) {
    for (int x = 0; x < k2p2; x++) {
      float accum[T];
      for (int ii = 0; ii < T; ii++) {
        accum[ii] = 0.0;
      }
      for (int ii = 0; ii < T; ii++) {
        for (int l = 0; l < n; l++) {
          if (!isnan(Yh[i+ii,l])) {
            accum[ii] +=
              Xh[y,l] * Xth[l,x];
          }
        }
      }
      for (int ii = 0; ii < T; ii++) {
        Xsqr[i+ii, y, x] = accum[ii];
      }
    }
  }
}
\end{minted}
\begin{center}
  Distribute loop
\end{center}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \begin{minted}[linenos,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {
  for (int y = 0; y < k2p2; y++) {
    for (int x = 0; x < k2p2; x++) {
      float accum[T];
      for (int ii = 0; ii < T; ii++) {
        accum[ii] = 0.0;
      }
      for (int l = 0; l < n; l++) {
        for (int ii = 0; ii < T; ii++) {
          if (!isnan(Yh[i+ii,l])) {
            accum[ii] +=
              Xh[y,l] * Xth[l,x];
          }
        }
      }
      for (int ii = 0; ii < T; ii++) {
        Xsqr[i+ii, y, x] = accum[ii];
      }
    }
  }
}
\end{minted}
\begin{center}
  Interchange again
\end{center}
\end{minipage}
\end{frame}

\begin{frame}[fragile]{Reaping the benefits}
\begin{minipage}{.55\textwidth}
  \centering
\begin{center}
  Hoist multiplication
\end{center}
  % NB! stripnl=false for padding
\begin{minted}[linenos,stripnl=false,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {
  for (int y = 0; y < k2p2; y++) {
    for (int x = 0; x < k2p2; x++) {
      float accum[T];
      for (int ii = 0; ii < T; ii++) {
        accum[ii] = 0.0;
      }
      for (int l = 0; l < n; l++) {
        float val = Xh[y,l] * Xth[l,x];
        for (int ii = 0; ii < T; ii++) {
          if (!isnan(Yh[i+ii,l])) {
            accum[ii] += val;
          }
        }
      }
      for (int ii = 0; ii < T; ii++) {
        Xsqr[i+ii, y, x] = accum[ii];
      }
    }
  }
}


\end{minted}
% NB! Don't remove blank lines near the end of the above minted environment
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
\begin{center}
  Utilize shared memory
\end{center}
\begin{minted}[linenos,fontsize=\tiny]{c}
for (int i = 0; i < m; i += T) {    //grid.x
  for (int y = 0; y < k2p2; y++) {  //block.y
    for (int x = 0; x < k2p2; x++) {//block.x
      float accum[T];
      for (int ii = 0; ii < T; ii++) {
        accum[ii] = 0.0;
      }
      for (int l = 0; l < n; l++) {
        float val = Xh[y,l] * Xth[l,x];
        __shared__ float Ysh[T];
        // Copy slice Yh[i:i+T, l] into Ysh
        for (int ii = 0; ii < T; ii++) {
          if (!isnan(Ysh[ii])) {
            accum[ii] += val;
          }
        }
      }
      for (int ii = 0; ii < T; ii++) {
        Xsqr[i+ii, y, x] = accum[ii];
      }
    }
  }
}
\end{minted}
\end{minipage}
\end{frame}

\begin{frame}[fragile]{Coalesced access and CUDA}
\begin{minted}[linenos,fontsize=\tiny]{c}
__global__ void bfast_step_2_tiled(float *Xh, float *Xth, float *Yth,
    float *Xsqr, int N, int n, int k2p2, int m)
{
  if (threadIdx.y >= k2p2 || threadIdx.x >= k2p2) { return; }
  float accum[STEP_2_TILE_SIZE];
  __shared__ float ysh[STEP_2_TILE_SIZE];
  for (int t = 0; t < STEP_2_TILE_SIZE; t++) { accum[t] = 0.0; }
  for (int i = 0; i < n; i++) {
    float val = Xh[IDX_2D(threadIdx.y, i, N)] * Xth[IDX_2D(i, threadIdx.x, k2p2)];
    int ysh_idx = IDX_2D(threadIdx.y, threadIdx.x, k2p2);
    if (ysh_idx < STEP_2_TILE_SIZE) {
      int y_row = blockIdx.x * STEP_2_TILE_SIZE + ysh_idx;
      if (y_row < m) { ysh[ysh_idx] = Yth[IDX_2D(i, y_row, N)]; }
      else { ysh[ysh_idx] = 0.0; }
    }
    __syncthreads();
    for (int t = 0; t < STEP_2_TILE_SIZE; t++) {
      if (!isnan(ysh[t])) { accum[t] += val; }
  } }
  for (int t = 0; t < STEP_2_TILE_SIZE; t++) {
    int mat_idx = blockIdx.x * STEP_2_TILE_SIZE + t;
    if (mat_idx < m) {
      Xsqr[mat_idx * k2p2 * k2p2 + IDX_2D(threadIdx.y, threadIdx.x, k2p2)] = accum[t];
} } }
\end{minted}
\end{frame}

\begin{frame}[fragile]{Runtimes on M1}
\begin{itemize}
  \item  \textbf{naive:} 24072 \textmu s
  \item  \textbf{hoisted:} 18107 \textmu s, \textasciitilde 1.3x speedup from naive
  \item  \textbf{shared memory:} 5832 \textmu s, \textasciitilde 4.1x speedup from naive
  \item  \textbf{coalesced access:} 2684 \textmu s, \textasciitilde 9x speedup from naive
\end{itemize}
\end{frame}

\section{Tiling in step 4}


\begin{frame}[fragile]{Futhark code and semantics}
\begin{minted}{haskell}
  let beta0  = map (matvecmul_row_filt Xh) Yh   -- bfast_step_4a
               |> intrinsics.opaque
  let beta   = map2 matvecmul_row Xinv beta0    -- bfast_step_4b
               |> intrinsics.opaque
  let y_preds= map (matvecmul_row Xt) beta      -- bfast_step_4c
               |> intrinsics.opaque
\end{minted}
Semantics:
\begin{description}
  \item[Step 4a:] \((X_hY_h^T)^T\) with filtering based on values in \(Y_h\).
  \item[Step 4b:] \(m\) matrix-vector multiplications.
    Each result vector is a row in the output matrix.
  \item[Step 4c:] Same as 4a but without filtering (simply mm-mult)
\end{description}
\end{frame}

\begin{frame}[fragile]{Tiling matrix-matrix multiplication (brief)}
  Matrix-matrix multiplication optimizations are applied to 4a and 4c.

  Naive mm-mult:
\begin{minted}{c}
for (int i = 0; i < M; i++) {
  for (int j = 0; j < N; j++) {
    float c = 0.0;
    for (int k = 0; k < U; k++) {
      c += A[i, k] * B[k, j];
    }
    C[i,j] = c;
  }
}
\end{minted}
\end{frame}

\begin{frame}[fragile]{Tiling matrix-matrix multiplication (brief)}
  Stripmine a bunch:
\begin{minted}{c}
for (int ii = 0; ii < M; ii += T) {
  for (int i = ii; i < min(M,ii+T); i++) {

    for (int jjj = 0; j < N; j += T*T) {
      for (int jj = jjj; jj < min(N,jjj+T*T); jj += T) {
        for (int j = jj; j < min(N,jj+T); j++) {

          float c = 0.0;
          for (int kk = 0; kk < U; kk += T) {
            for (int k = kk; k < min(U,kk+T); k++) {
              c += A[i, k] * B[k, j];
            }
          }
          C[i,j] = c;

    } } }

} }
\end{minted}
\end{frame}

\begin{frame}[fragile]{Tiling matrix-matrix multiplication (brief)}
  Interchange i-loop inwards, distribute it (array expansion on \texttt{c}),
  interchange i-loop further inwards.
\begin{minted}{c}
for (int ii = 0; ii < M; ii += T) {
  for (int jjj = 0; j < N; j += T*T) {
    for (int jj = jjj; jj < min(N,jjj+T*T); jj += T) {
      for (int j = jj; j < min(N,jj+T); j++) {
        float c[T];
        for (int i = ii; i < min(M,ii+T); i++) {
          c[i] = 0.0;
        }
        for (int kk = 0; kk < U; kk += T) {
          for (int k = kk; k < min(U,kk+T); k++) {
            for (int i = ii; i < min(M,ii+T); i++) {
              c[i] += A[i, k] * B[k, j];
            }
          }
        }
        for (int i = ii; i < min(M,ii+T); i++) {
          C[i,j] = c[i];
        }
} } } }
\end{minted}
\end{frame}

\begin{frame}[fragile]{Tiling matrix-matrix multiplication (brief)}
  Reap the benefits: Hoist multiplication, read elements from \(A\) to shared
  memory (avoids uncoalesced access).
\begin{minted}{c}
for (int ii = 0; ii < M; ii += T) {                     // grid.y
  for (int jjj = 0; j < N; j += T*T) {                  // grid.x
    for (int jj = jjj; jj < min(N,jjj+T*T); jj += T) {  // block.y
      for (int j = jj; j < min(N,jj+T); j++) {          // block.x
        float c[T];
        for (int i = ii; i < min(M,ii+T); i++) { c[i] = 0.0; }
        for (int kk = 0; kk < U; kk += T) {
          // Copy slice A[ii:ii+T, kk:kk+T] to shared mem (Ash)
          for (int k = kk; k < min(U,kk+T); k++) {
            float val = Ash[i, k] * B[k, j];
            for (int i = ii; i < min(M,ii+T); i++) { // unroll
              c[i] += val;
            }
          }
        }
        for (int i = ii; i < min(M,ii+T); i++) { C[i,j] = c[i]; }
} } } }
\end{minted}
\end{frame}

\begin{frame}[fragile]{Regarding 4b}
  Naive:
\begin{minted}{c}
for (int i = 0; i < m; i++) {         // grid.x
  for (int j = 0; j < k2p2; j++) {    // block.x
    float accum = 0.0;
    for (int k = 0; k < k2p2; k++) {  // sequential
      accum += Xinv[i, j, k] * beta0[i, k];
    }
    beta[i, j] = accum;
  }
}
\end{minted}
\begin{itemize}
  \item
    Apart from \texttt{beta0}, nothing is accessed twice.
  \item
    No performance gain from copying row from \texttt{beta0} to shared memory
    (likely due to \texttt{k2p2} being small).
  \item
    Note: Uncoalesced access in \texttt{Xinv}.
\end{itemize}
\end{frame}

\section{Performance portability and the cost of transposition}

\begin{frame}[fragile]{Not transposing \texttt{Y}}
Experiment: Avoid transposing \texttt{Y}.
How to accomplish this?
\begin{itemize}
  \item In step 2: Apply tiling transformations, hoist global memory acceses,
    and apply shared memory.
  \item In step 4a: Observe that \( (X_hY_h^T)^T = Y_hX_h^T \)
\end{itemize}
\end{frame}

\begin{frame}[fragile]{\texttt{bfast-opt} vs \texttt{bfast-opt-alt} runtimes on M1}
\begin{minipage}{.55\textwidth}
\centering
\begin{minted}{text}
bfast-opt (average of 5000 runs):

bfast_step_1_run             12.30 µs
transpose_X                  11.58 µs
transpose_Y                1214.46 µs
bfast_step_2_tiled_run     2697.29 µs
bfast_step_3_run           2625.04 µs
bfast_step_4a_tiled_run     715.22 µs
untranspose_beta0            75.84 µs
bfast_step_4b_run           502.77 µs
bfast_step_4c_tiled_run    1016.98 µs
bfast_step_5_run           2923.92 µs
bfast_step_6_reuse_run     2018.00 µs
bfast_step_7a_run           484.01 µs
bfast_step_7b_run            10.17 µs
bfast_step_8_opt2_run      1742.87 µs


Total runtime             16050.43 µs
\end{minted}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{minted}{text}
bfast-opt-alt (average of 5000 runs):

bfast_step_1_run               12.48 µs
transpose_X                    10.96 µs

bfast_step_2_shr_run         5827.21 µs
bfast_step_3_run             2638.40 µs
bfast_step_4a_flipped_run    2489.38 µs

bfast_step_4b_run             505.05 µs
bfast_step_4c_tiled_run      1020.06 µs
bfast_step_5_run             2899.32 µs
bfast_step_6_reuse_run       1998.03 µs
bfast_step_7a_run             477.29 µs
bfast_step_7b_run              10.02 µs
bfast_step_8_opt2_run        1730.79 µs


Total runtime               19618.98 µs
\end{minted}
\end{minipage}
\end{frame}

\begin{frame}[fragile]{\texttt{bfast-opt} vs \texttt{bfast-opt-alt} runtimes on M2}
\begin{minipage}{.55\textwidth}
\centering
\begin{minted}{text}
bfast-opt (average of 5000 runs):

bfast_step_1_run              7.88 µs
transpose_X                   7.39 µs
transpose_Y                1632.40 µs
bfast_step_2_tiled_run     2422.79 µs
bfast_step_3_run           2791.03 µs
bfast_step_4a_tiled_run     666.09 µs
untranspose_beta0            78.35 µs
bfast_step_4b_run           380.87 µs
bfast_step_4c_tiled_run     936.56 µs
bfast_step_5_run           3281.51 µs
bfast_step_6_reuse_run     2040.84 µs
bfast_step_7a_run           336.99 µs
bfast_step_7b_run             6.87 µs
bfast_step_8_opt2_run      1739.55 µs


Total runtime             16329.13 µs
\end{minted}
\end{minipage}%
\begin{minipage}{.5\textwidth}
\centering
\begin{minted}{text}
bfast-opt-alt (average of 5000 runs):

bfast_step_1_run                7.86 µs
transpose_X                     7.44 µs

bfast_step_2_shr_run         2625.80 µs
bfast_step_3_run             2790.07 µs
bfast_step_4a_flipped_run    1625.94 µs

bfast_step_4b_run             380.64 µs
bfast_step_4c_tiled_run       936.11 µs
bfast_step_5_run             3280.87 µs
bfast_step_6_reuse_run       2038.85 µs
bfast_step_7a_run             336.83 µs
bfast_step_7b_run               6.81 µs
bfast_step_8_opt2_run        1740.31 µs


Total runtime               15777.55 µs
\end{minted}
\end{minipage}
\end{frame}



