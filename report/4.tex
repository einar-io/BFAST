\subsection{Kernel 4}

We have chosen to split step 4 seen in \autoref{fut:kernel4} into 3 different
kernels, which we will investigate in the following three subsections.

\begin{figure}[H]
    \centering
    \ehaskell[firstline=127,lastline=138]{../src/fut-handout/bfast-distrib.fut}
    \caption{The original Futhark functions representing step 4.}
    \label{fut:kernel4}
\end{figure}

\subsubsection{Kernel 4a:  Matrix-matrix multiplication}

\begin{figure}[H]
    \centering
    \ehaskell[firstline=100,lastline=110]{../src/fut-handout/bfast-distrib.fut}
    \caption{}
    \label{fut:kernel4a}
\end{figure}


Line 133 in the Futhark code in \autoref{fut:kernel4} corresponds to a matrix
matrix multiplication with an additional filtering. 

This kernel is interesting because it can be implemented in different ways
resulting in different tradeoffs. Summarized we can either 

\begin{enumerate*}
    \item minimize the runnning time for this single kernel, or
    \item minimize the running time over all kernels. But,
    \item we cannot optimize both.
\end{enumerate*}

In the end we want to optimize (2).

Assuming floating-point numbers work as commutative scalars, the transpose
identity for a matrix product of such elements is given by:

\[ (AB)^T = B^T A^T. \]

What this kernel does, besides the filtering, corresponds to calculating

\[ \texttt{beta0} = (XY^T)^T = YX^T \]
\[ \texttt{beta0}^T = XY^T \]

according to above identity. So we need to compare the cost of transposing
\(X\), \(Y\) and \(\texttt{beta0}^T\) as well as whether the following kernels
are faster with the transposed or untransposed versions of X, Y or
\texttt{beta0}. 

After different attempts found in \texttt{src/bfast\_step\_4a.cu} we found the
shortest running time for all steps simply to use the block and register tiled
version of matrix matrix multiplication from the weekly assignment. This can be
seen in \autoref{cuda:kernel4a}. Notice the added conditional on line 59 which
implements the filtering.

The loop unrolling and register tiling is performed by the \texttt{\#pragma
unroll} on line 37, 57 and 68 of \autoref{cuda:kernel4a}.


\begin{figure}[H]
    \centering
    \ecuda[firstline=24,lastline=74]{../src/kernels/bfast_step_4a.cu}
    \caption{CUDA kernel for block and register tiled version of matrix matrix
    multiplication with filtering.}
    \label{cuda:kernel4a}
\end{figure}


\subsubsection{Kernel 4b:  Matrix-matrix multiplication}

\begin{figure}[H]
    \centering
    \ecuda[firstline=131,lastline=164]{../src/kernels/bfast_others.cu}
    \caption{CUDA kernel for calculating \texttt{beta}.}
    \label{cuda:kernel4b}
\end{figure}


\subsubsection{Kernel 4c:  Matrix-matrix multiplication}

\begin{figure}[H]
    \centering
    \ecuda[firstline=24,lastline=74]{../src/kernels/bfast_step_4c.cu}
    \caption{CUDA kernel for block and register tiled version of matrix matrix multiplication.}
    \label{cuda:kernel4c}
\end{figure}



